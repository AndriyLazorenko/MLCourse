{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792f936b-d98a-4480-8be3-eb9587c505f1",
   "metadata": {},
   "source": [
    "![title.png](images/ML_08_title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196c957",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "### 1. ROC and AUC\n",
    "### 2. Cross validation and k-fold\n",
    "### 3. Grid search of hyperparameters\n",
    "### 4. Gradient boosting intuition\n",
    "### 5. Catboost: ordered target encoding\n",
    "### 6. Catboost: building trees\n",
    "### 7. Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892c029",
   "metadata": {},
   "source": [
    "## 1. ML fundamentals: ROC and AUC\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) vs. the False Positive Rate (FPR) for different classification thresholds.\n",
    "The Area Under the Curve (AUC) is given by:\n",
    "\n",
    "$$\\text{TPR} = \\frac{TP}{TP + FN}$$  \n",
    "$$\\text{FPR} = \\frac{FP}{FP + TN}$$  \n",
    "$$\\text{AUC} = \\int_0^1 TPR(FPR) \\, dFPR$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55030b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "y_score = clf.predict_proba(X_test)[:, 1] # attention here!\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83da544",
   "metadata": {},
   "source": [
    "**Practical application:** ROC and AUC are widely used to evaluate binary classifiers, especially when class imbalance exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3b198",
   "metadata": {},
   "source": [
    "## 2. ML fundamentals: cross validation and k-fold\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Cross-validation splits the dataset into $k$ folds. Each fold is used once as a validation set while the remaining $k-1$ folds form the training set.\n",
    "The $k$-fold CV estimate is:\n",
    "$$CV_k = \\frac{1}{k} \\sum_{i=1}^{k} M_i$$\n",
    "where $M_i$ is the performance metric for fold $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa59c4-e438-40c2-b5a8-1a07b069bb19",
   "metadata": {},
   "source": [
    "![title.png](images/kfold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bf6a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per fold: [1.         1.         0.93333333 0.96666667 0.96666667]\n",
      "Mean score: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=kf)\n",
    "print('Scores per fold:', scores)\n",
    "print('Mean score:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575ab13",
   "metadata": {},
   "source": [
    "**Practical application:** K-fold cross-validation is used to obtain a more reliable estimate of model performance and reduce overfitting risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180313c",
   "metadata": {},
   "source": [
    "## 3. ML fundamentals: grid search of hyperparameters\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Grid search tests all possible combinations of given hyperparameters and selects the one yielding the best performance.\n",
    "Formally, it searches:\n",
    "$$ \\arg\\max_{\\theta \\in \\Theta} M(\\theta) $$\n",
    "where $M$ is the performance metric and $\\Theta$ is the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8a93b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'kernel': 'rbf'}\n",
      "Best score: 0.870119101462385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8368d",
   "metadata": {},
   "source": [
    "**Practical application:** Grid search is useful for tuning hyperparameters to maximize model performance. Also check out Bayesian search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed60be",
   "metadata": {},
   "source": [
    "## 4. Gradient boosting intuition\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Gradient boosting builds an additive model by sequentially fitting weak learners to the residual errors of prior models.\n",
    "The update at iteration $m$ is:\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$$\n",
    "where $\\nu$ is the learning rate and $h_m$ is the weak learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bd377-9665-4ff5-9d6d-a623901bd87b",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm — Step-by-Step Explanation with Tables\n",
    "\n",
    "Consider a simple regression problem with the following dataset:\n",
    "\n",
    "| Row | Feature $x$ | Target $y$ |\n",
    "|------|---------------|--------------|\n",
    "| 1    | 1             | 4            |\n",
    "| 2    | 2             | 5            |\n",
    "| 3    | 3             | 7            |\n",
    "| 4    | 4             | 10           |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 — Initialization\n",
    "\n",
    "- Start with a constant prediction, usually the mean of the target values:\n",
    "\n",
    "$$\n",
    "F_0 = \\frac{4 + 5 + 7 + 10}{4} = 6.5\n",
    "$$\n",
    "\n",
    "| Row | Target $y$ | Initial Prediction $F_0$ | Residual $r_1 = y - F_0$ |\n",
    "|------|--------------|----------------------------|-----------------------------|\n",
    "| 1    | 4            | 6.5                        | $4 - 6.5 = -2.5$          |\n",
    "| 2    | 5            | 6.5                        | $5 - 6.5 = -1.5$          |\n",
    "| 3    | 7            | 6.5                        | $7 - 6.5 = 0.5$           |\n",
    "| 4    | 10           | 6.5                        | $10 - 6.5 = 3.5$          |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2 — Fit the first weak learner (tree) on residuals\n",
    "\n",
    "- Fit a simple regression tree (e.g., depth 1 or 2) to predict residuals $r_1$.\n",
    "\n",
    "| Row | Feature $x$ | Residual $r_1$ | Leaf assignment (split $x < 2.5$) |\n",
    "|------|---------------|------------------|-------------------------------------|\n",
    "| 1    | 1             | -2.5             | Left leaf                          |\n",
    "| 2    | 2             | -1.5             | Left leaf                          |\n",
    "| 3    | 3             | 0.5              | Right leaf                         |\n",
    "| 4    | 4             | 3.5              | Right leaf                         |\n",
    "\n",
    "- Compute leaf values as average residuals:\n",
    "\n",
    "| Leaf       | Residuals         | Leaf value (average residual) |\n",
    "|------------|-------------------|-------------------------------|\n",
    "| Left leaf  | -2.5, -1.5        | $\\frac{-2.5 + (-1.5)}{2} = -2.0$   |\n",
    "| Right leaf | 0.5, 3.5          | $\\frac{0.5 + 3.5}{2} = 2.0$         |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3 — Update predictions\n",
    "\n",
    "- Update predictions by adding scaled leaf values multiplied by learning rate $\\nu = 0.1$:\n",
    "\n",
    "$$\n",
    "F_1 = F_0 + \\nu \\times h_1(x)\n",
    "$$\n",
    "\n",
    "| Row | Previous Prediction $F_0$ | Leaf Value | Update $\\nu \\times \\text{Leaf Value}$ | New Prediction $F_1$       |\n",
    "|------|-----------------------------|------------|------------------------------------------|------------------------------|\n",
    "| 1    | 6.5                         | -2.0       | $0.1 \\times -2.0 = -0.2$              | $6.5 - 0.2 = 6.3$           |\n",
    "| 2    | 6.5                         | -2.0       | -0.2                                    | 6.3                          |\n",
    "| 3    | 6.5                         | 2.0        | $0.1 \\times 2.0 = 0.2$                | $6.5 + 0.2 = 6.7$           |\n",
    "| 4    | 6.5                         | 2.0        | 0.2                                     | 6.7                          |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4 — Compute new residuals\n",
    "\n",
    "$$\n",
    "r_2 = y - F_1\n",
    "$$\n",
    "\n",
    "| Row | Target $y$ | Prediction $F_1$ | Residual $r_2 = y - F_1$ |\n",
    "|------|--------------|--------------------|-----------------------------|\n",
    "| 1    | 4            | 6.3                | $-2.3$                    |\n",
    "| 2    | 5            | 6.3                | $-1.3$                    |\n",
    "| 3    | 7            | 6.7                | $0.3$                     |\n",
    "| 4    | 10           | 6.7                | $3.3$                     |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5 — Fit the second weak learner on new residuals\n",
    "\n",
    "- Fit another tree to predict residuals $r_2$ (splitting $x < 3.5$):\n",
    "\n",
    "| Row | Feature $x$ | Residual $r_2$ | Leaf assignment |\n",
    "|------|---------------|------------------|-----------------|\n",
    "| 1    | 1             | -2.3             | Left leaf       |\n",
    "| 2    | 2             | -1.3             | Left leaf       |\n",
    "| 3    | 3             | 0.3              | Left leaf       |\n",
    "| 4    | 4             | 3.3              | Right leaf      |\n",
    "\n",
    "- Compute leaf values:\n",
    "\n",
    "| Leaf       | Residuals           | Leaf Value (average)     |\n",
    "|------------|---------------------|-------------------------|\n",
    "| Left leaf  | -2.3, -1.3, 0.3     | $\\frac{-2.3 -1.3 + 0.3}{3} = -1.1$ |\n",
    "| Right leaf | 3.3                 | 3.3                     |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6 — Update predictions again\n",
    "\n",
    "| Row | Previous Prediction $F_1$ | Leaf Value | Update $\\nu \\times \\text{Leaf Value}$ | New Prediction $F_2$       |\n",
    "|------|-----------------------------|------------|------------------------------------------|------------------------------|\n",
    "| 1    | 6.3                         | -1.1       | $-0.11$                               | $6.3 - 0.11 = 6.19$         |\n",
    "| 2    | 6.3                         | -1.1       | -0.11                                  | 6.19                         |\n",
    "| 3    | 6.7                         | -1.1       | -0.11                                  | 6.59                         |\n",
    "| 4    | 6.7                         | 3.3        | 0.33                                   | 7.03                         |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 7 — Repeat until convergence or stopping criteria\n",
    "\n",
    "- Continue fitting new trees on residuals and updating predictions until performance is satisfactory or max iterations reached.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary table of each iteration\n",
    "\n",
    "| Iteration | Action                              | Outcome                          |\n",
    "|-----------|-----------------------------------|---------------------------------|\n",
    "| 0         | Initialize $F_0$                | Constant prediction (mean target)|\n",
    "| 1         | Fit tree on residuals $r_1$    | Learn to correct initial errors  |\n",
    "| 2         | Update predictions to $F_1$    | Improved predictions             |\n",
    "| 3         | Calculate new residuals $r_2$  | New errors to correct            |\n",
    "| 4         | Fit next tree on residuals $r_2$| Further refine model            |\n",
    "| ...       | Repeat                           | Model gradually improves         |\n",
    "\n",
    "---\n",
    "\n",
    "This table-based stepwise view helps understand the mechanics behind gradient boosting — iteratively learning and correcting residual errors with weak learners (trees).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256373f7",
   "metadata": {},
   "source": [
    "## Catboost\n",
    "\n",
    "1. **Random Permutation of Data**  \n",
    "   The training data is randomly shuffled to create a specific order. This order is essential to prevent target leakage during encoding.\n",
    "\n",
    "2. **Ordered Target Encoding of Categorical Features**  \n",
    "   For each categorical feature, CatBoost computes the target encoding **on-the-fly** for each example, using only the target values from examples that come *before* it in the shuffled order.  \n",
    "   This means when encoding example $i$, only data from examples $1, 2, \\ldots, i-1$ is used.\n",
    "\n",
    "\n",
    "#### Ordered Target Encoding Example (Step-by-Step)\n",
    "\n",
    "Suppose we have the following ordered data:\n",
    "\n",
    "| Row | Category | Target (y) |\n",
    "|------|----------|------------|\n",
    "| 1    | A        | 1          |\n",
    "| 2    | B        | 0          |\n",
    "| 3    | A        | 0          |\n",
    "| 4    | B        | 1          |\n",
    "| 5    | A        | 1          |\n",
    "\n",
    "---\n",
    "\n",
    "### Calculating ordered target encoding for each row:\n",
    "\n",
    "We use the formula:\n",
    "\n",
    "$$\n",
    "\\text{encoding}_i = \\frac{\\sum_{j < i,\\, category_j = category_i} y_j + a \\cdot P}{N_i + a}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $N_i$ = number of previous rows with the same category  \n",
    "- $a$ = smoothing parameter (e.g., 1)  \n",
    "- $P$ = global mean target = $\\frac{1+0+0+1+1}{5} = 0.6$\n",
    "\n",
    "---\n",
    "\n",
    "| Row | Category | Target | Previous targets (same category) | Encoding Calculation                                    | Encoding |\n",
    "|-------|----------|--------|----------------------------------|-------------------------------------------------------|----------|\n",
    "| 1     | A        | 1      | None                             | $\\frac{0 + 1 \\times 0.6}{0 + 1} = 0.6$              | 0.6      |\n",
    "| 2     | B        | 0      | None                             | $\\frac{0 + 1 \\times 0.6}{0 + 1} = 0.6$              | 0.6      |\n",
    "| 3     | A        | 0      | Row 1 target = 1                 | $\\frac{1 + 1 \\times 0.6}{1 + 1} = \\frac{1.6}{2} = 0.8$| 0.8      |\n",
    "| 4     | B        | 1      | Row 2 target = 0                 | $\\frac{0 + 1 \\times 0.6}{1 + 1} = \\frac{0.6}{2} = 0.3$| 0.3      |\n",
    "| 5     | A        | 1      | Rows 1,3 targets = 1,0          | $\\frac{1 + 0 + 1 \\times 0.6}{2 + 1} = \\frac{1.6}{3} \\approx 0.53$ | 0.53     |\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- For **row 1**, no previous examples with category \"A\", so encoding is just the prior \\(P=0.6\\) weighted by smoothing.  \n",
    "- For **row 3**, previous row with \"A\" is row 1 with target 1, so encoding is average of \\([1]\\) plus smoothing.  \n",
    "- For **row 5**, previous rows with \"A\" are rows 1 and 3 with targets \\([1,0]\\), so encoding is their average plus smoothing.  \n",
    "- Smoothing stabilizes the encoding especially for categories with few prior examples.\n",
    "\n",
    "---\n",
    "\n",
    "This approach prevents target leakage by using only **past information** when encoding each example.\n",
    "\n",
    "\n",
    "4. **Iterative Tree Building**  \n",
    "   Trees are built one at a time. For each new tree:  \n",
    "   - Use the ordered target encoded features.  \n",
    "   - Fit the tree to the current residuals (pseudo-residuals) based on the model so far.\n",
    "\n",
    "### CatBoost Iterative Tree-Building — Step-by-Step Intuition\n",
    "\n",
    "Imagine we want to train a CatBoost model on a small dataset. The training proceeds in **iterations**, where each iteration builds one tree to improve the model by correcting previous errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dataset (simplified)\n",
    "\n",
    "| Row | Features (numeric + encoded categorical) | Target (y) |\n",
    "|------|------------------------------------------|------------|\n",
    "| 1    | [1.5, 0.6]                               | 1          |\n",
    "| 2    | [2.3, 0.4]                               | 0          |\n",
    "| 3    | [1.1, 0.8]                               | 1          |\n",
    "| 4    | [3.0, 0.2]                               | 0          |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 — Initialization\n",
    "\n",
    "- Start with a constant prediction, usually the log-odds of the positive class (or mean target for regression).\n",
    "\n",
    "| Row | Initial Prediction $F_0$ | Residual (pseudo-residual) $r_1 = y - \\hat{y}$ |\n",
    "|------|----------------------------|--------------------------------------------------|\n",
    "| 1    | 0.4                        | $1 - 0.4 = 0.6$                              |\n",
    "| 2    | 0.4                        | $0 - 0.4 = -0.4$                               |\n",
    "| 3    | 0.4                        | $1 - 0.4 = 0.6$                                |\n",
    "| 4    | 0.4                        | $0 - 0.4 = -0.4$                               |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2 — Build the first tree on residuals\n",
    "\n",
    "- Fit a small decision tree (e.g., max depth 1 or 2) to predict residuals $r_1$.\n",
    "\n",
    "##### Symmetric vs asymmetric trees\n",
    "- In **classic asymmetric trees** (used by many decision tree algorithms):\n",
    "  - Each node **independently chooses** the best feature and threshold to split on.\n",
    "  - For example:\n",
    "    - The root node might split on feature A < 1.5.\n",
    "    - The left child node might split on feature B < 3.0.\n",
    "    - The right child node might split on feature C < 2.0.\n",
    "  - This creates an **irregular and potentially unbalanced** tree structure.\n",
    "\n",
    "- In **symmetric (oblivious) trees** (used by CatBoost):\n",
    "  - All nodes at the **same depth level** use the **same feature and threshold** to split.\n",
    "  - For example:\n",
    "    - At depth 1, **every node** splits on feature A < 1.5.\n",
    "    - At depth 2, **every node** splits on feature B < 3.0.\n",
    "  - This produces a **perfectly balanced and symmetric** tree.\n",
    "\n",
    "---\n",
    "\n",
    "##### Visual example\n",
    "\n",
    "| Classic Asymmetric Tree              | Symmetric (Oblivious) Tree            |\n",
    "|------------------------------------|-------------------------------------|\n",
    "| Root splits on feature A < 1.5     | Root splits on feature A < 1.5      |\n",
    "| Left child splits on feature B < 3 | Left child splits on feature B < 3  |\n",
    "| Right child splits on feature C < 2| Right child splits on feature B < 3 |\n",
    "| Children split on different features and thresholds | All nodes on depth 2 split on feature B < 3 |\n",
    "\n",
    "---\n",
    "\n",
    "##### Why is symmetric splitting important?\n",
    "\n",
    "- **Faster prediction:** Balanced trees allow simpler and more parallelizable prediction logic.\n",
    "- **Reduced overfitting:** Restricting splits limits model complexity.\n",
    "- **Efficient implementation:** Enables optimized storage and computation.\n",
    "\n",
    "---\n",
    "\n",
    "##### How this fits in the CatBoost example\n",
    "\n",
    "- When the example shows a split on feature 1 at threshold 2.0, symmetric trees mean:\n",
    "  - **All nodes at that level split on feature 1 < 2.0**, not just one node.\n",
    "  - This results in a balanced binary tree where every path is defined by the same set of splits applied consistently.\n",
    "\n",
    "- In this simplified example, the tree splits on **feature 1 at threshold 2.0**:\n",
    "\n",
    "| Row | Feature 1 | Residual $r_1$ | Leaf assignment |\n",
    "|------|-----------|------------------|-----------------|\n",
    "| 1    | 1.5       | 0.6              | Left leaf       |\n",
    "| 2    | 2.3       | -0.4             | Right leaf      |\n",
    "| 3    | 1.1       | 0.6              | Left leaf       |\n",
    "| 4    | 3.0       | -0.4             | Right leaf      |\n",
    "\n",
    "- Compute leaf values as average residuals:\n",
    "\n",
    "| Leaf       | Residuals        | Leaf Value (average) |\n",
    "|------------|------------------|---------------------|\n",
    "| Left leaf  | 0.6, 0.6         | 0.6                 |\n",
    "| Right leaf | -0.4, -0.4       | -0.4                |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3 — Update model predictions\n",
    "\n",
    "- Add scaled leaf values to predictions (learning rate $\\nu=0.1$):\n",
    "\n",
    "| Row | Old Prediction $F_0$ | Leaf Value | Update $\\nu \\times \\text{Leaf Value}$ | New Prediction $F_1$ |\n",
    "|------|------------------------|------------|------------------------------------------|------------------------|\n",
    "| 1    | 0.4                    | 0.6        | 0.06                                     | 0.46                   |\n",
    "| 2    | 0.4                    | -0.4       | -0.04                                    | 0.36                   |\n",
    "| 3    | 0.4                    | 0.6        | 0.06                                     | 0.46                   |\n",
    "| 4    | 0.4                    | -0.4       | -0.04                                    | 0.36                   |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4 — Compute new residuals\n",
    "\n",
    "| Row | Target $y$ | Prediction $F_1$ | New Residual $r_2 = y - F_1$ |\n",
    "|------|--------------|-------------------|---------------------------------|\n",
    "| 1    | 1            | 0.46              | 0.54                            |\n",
    "| 2    | 0            | 0.36              | -0.36                           |\n",
    "| 3    | 1            | 0.46              | 0.54                            |\n",
    "| 4    | 0            | 0.36              | -0.36                           |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5 — Build second tree on new residuals and repeat\n",
    "\n",
    "- Fit a new tree on residuals $r_2$ to further improve predictions.\n",
    "- Continue this process iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Iteration | Action                                  | Purpose                                      |\n",
    "|-----------|-----------------------------------------|----------------------------------------------|\n",
    "| 0         | Initialize with constant prediction     | Baseline prediction before learning           |\n",
    "| 1         | Fit tree on residuals $r_1$           | Learn to correct errors of baseline            |\n",
    "| 2         | Update predictions with tree output     | Improve model accuracy                          |\n",
    "| 3         | Calculate new residuals $r_2$          | Identify remaining errors to fix                |\n",
    "| 4         | Fit next tree on residuals $r_2$       | Continue refining the model                      |\n",
    "| ...       | Repeat until stopping criteria           | Achieve desired performance                      |\n",
    "\n",
    "---\n",
    "\n",
    "This iterative refinement with small trees is how CatBoost (and other gradient boosting methods) gradually build a strong predictive model.\n",
    "\n",
    "6. **Gradient Calculation and Model Update**  \n",
    "   Update the ensemble model by adding the new tree scaled by the learning rate.\n",
    "\n",
    "7. **Repeat for Many Iterations**  \n",
    "   Repeat the encoding, residual computation, and tree building for the desired number of boosting rounds or until convergence.\n",
    "\n",
    "8. **Prediction on New Data**  \n",
    "   For new data, CatBoost uses techniques such as averaging over multiple permutations to generate stable ordered target encodings and make predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Why CatBoost’s approach is special\n",
    "\n",
    "- Embeds **ordered target encoding** directly in training to avoid target leakage while leveraging target statistics.  \n",
    "- Uses **symmetric trees** and efficient data structures for faster training and better generalization.  \n",
    "- Handles missing data and categorical features without extra preprocessing.  \n",
    "- Supports GPU acceleration for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb18926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier AUC: 0.9951\n",
      "CatBoostClassifier AUC: 0.9968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred_gbc = gbc.predict_proba(X_test)[:, 1]\n",
    "auc_gbc = roc_auc_score(y_test, y_pred_gbc)\n",
    "\n",
    "# CatBoost Classifier\n",
    "cbc = CatBoostClassifier(random_state=42, verbose=0)\n",
    "cbc.fit(X_train, y_train)\n",
    "y_pred_cbc = cbc.predict_proba(X_test)[:, 1]\n",
    "auc_cbc = roc_auc_score(y_test, y_pred_cbc)\n",
    "\n",
    "print(f\"GradientBoostingClassifier AUC: {auc_gbc:.4f}\")\n",
    "print(f\"CatBoostClassifier AUC: {auc_cbc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6996e-7415-4d4d-8a73-74868430541a",
   "metadata": {},
   "source": [
    "### Which Algorithm Works Best in Kaggle Competitions and Competitive ML?\n",
    "\n",
    "In the realm of Kaggle competitions and competitive machine learning, **Gradient Boosting** algorithms based on decision trees are among the most powerful and popular methods. Among them, **XGBoost**, **CatBoost**, and traditional **Gradient Boosting (like sklearn’s implementation)** are widely used.\n",
    "\n",
    "- **XGBoost** has long been the de facto standard due to its speed, scalability, and strong performance on tabular data. It introduced important algorithmic and system optimizations (e.g., regularization, parallelization, approximate split finding) that made it a favorite among competitors.\n",
    "\n",
    "- **CatBoost**, developed by Yandex, has gained popularity more recently for its state-of-the-art handling of categorical features without the need for extensive preprocessing, its **ordered boosting** technique that reduces target leakage, and its use of symmetric trees, which lead to faster training and better generalization in many scenarios.\n",
    "\n",
    "- **Traditional Gradient Boosting** implementations (e.g., `sklearn.ensemble.GradientBoostingClassifier`) are simpler and easier to use but usually lag behind XGBoost and CatBoost in performance and efficiency, especially on large or complex datasets.\n",
    "\n",
    "In general, **CatBoost and XGBoost tend to outperform traditional Gradient Boosting** in competitions. Between CatBoost and XGBoost, the best choice depends on dataset characteristics, feature types, and tuning efforts. CatBoost often excels when datasets contain many categorical variables or require reduced parameter tuning, while XGBoost is very flexible and highly optimized for numerical data.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm Comparison Table\n",
    "\n",
    "| Feature / Aspect                | Gradient Boosting (sklearn)      | XGBoost                          | CatBoost                         |\n",
    "|-------------------------------|---------------------------------|---------------------------------|---------------------------------|\n",
    "| **Handling of categorical features** | Requires manual encoding (one-hot, label encoding) | Requires encoding (though has some support) | Native categorical feature support without preprocessing |\n",
    "| **Training speed & scalability**     | Moderate, slower on large data  | Fast, highly optimized (parallel & distributed) | Fast with GPU support, efficient due to symmetric trees |\n",
    "| **Prediction speed**                  | Moderate                       | Fast                            | Very fast due to symmetric trees |\n",
    "| **Overfitting control**               | Basic regularization options   | Advanced regularization (L1, L2) | Ordered boosting reduces target leakage and overfitting |\n",
    "| **Ease of use**                      | Very easy, standard API        | More parameters, requires tuning | User-friendly, fewer hyperparameters to tune |\n",
    "| **Handling of target leakage**       | No special measures            | No special measures             | Ordered boosting explicitly handles target leakage |\n",
    "| **Popularity in competitions**       | Less common nowadays           | Very popular, widely used       | Increasing popularity, especially with categorical data |\n",
    "| **Interpretability**                 | Good                          | Good                           | Good, with tools like SHAP integration |\n",
    "| **GPU support**                     | Limited                       | Excellent                      | Excellent                       |\n",
    "\n",
    "---\n",
    "\n",
    "Overall, **CatBoost and XGBoost represent state-of-the-art gradient boosting algorithms widely preferred in competitive machine learning**, with CatBoost standing out for categorical data and ease of use, and XGBoost for raw speed and flexibility.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
